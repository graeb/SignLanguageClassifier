{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from os import cpu_count\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import wandb\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login <your key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'Final accuracy',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'model': {\n",
    "        'value': 'squ'\n",
    "        },\n",
    "    'pretrained': {\n",
    "        'value': False\n",
    "        },\n",
    "    'optimizer': {\n",
    "        'value': 'adam'\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'value': 32\n",
    "        },\n",
    "    'epochs': {\n",
    "        'value': 400\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'value' : 0.0001\n",
    "        },\n",
    "    'momentum': {\n",
    "        'value' : 0.9\n",
    "        },\n",
    "    'scheduler': {\n",
    "        'value' : 'ReduceLROnPlateau'\n",
    "        },\n",
    "\n",
    "    'weight_decay': {\n",
    "        'value': 0.0001\n",
    "        },\n",
    "    'augmentation': {\n",
    "        'values': ['mix', 'rand']\n",
    "        },\n",
    "    }\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def train(config=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    class_names = ['A','B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y']\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, resume=True):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        loader, dataset_sizes = build_dataset(config.batch_size, config.augmentation)# config.img_size)\n",
    "        network = build_network(config.model, config.pretrained, )#dropout = config.dropout,) #batch_norm = config.batch_normalization)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate, config.momentum, config.weight_decay)\n",
    "        scheduler = build_scheduler(scheduler_type=config.scheduler, optimizer_ft=optimizer,)# step_sched = config.step_sched, gamma_sched= config.gamma_sched)\n",
    "        \n",
    "        since = time.time()\n",
    "        # best_model_wts = copy.deepcopy(network.state_dict())\n",
    "        # best_acc = 0.0\n",
    "        \n",
    "        # for epoch in range(config.epochs):\n",
    "        best_model = train_epoch(network, loader, optimizer, scheduler, dataset_sizes, scheduler_type=config.scheduler, num_epochs=config.epochs,) # model, dataloaders, optimizer, scheduler, dataset_sizes, criterion = nn.CrossEntropyLoss()\n",
    "        final_loss, final_acc = test_model(best_model, loader, dataset_sizes, ) # (network, loader, dataset_sizes, criterion = nn.CrossEntropyLoss())\n",
    "        wandb.log({\"Final loss\": final_loss, \"Final accuracy\": final_acc})\n",
    "        top_pred_ids, ground_truth_class_ids = confusion_matrix_pass(best_model, loader)\n",
    "        wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "            preds=top_pred_ids, y_true=ground_truth_class_ids,\n",
    "            class_names=class_names)})\n",
    "             \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size, augmentation, img_size = (256,256), ): #\n",
    "    from squareImage import squareImage\n",
    "    \n",
    "    if augmentation == 'rand':\n",
    "        augmentation_transform = transforms.RandAugment()\n",
    "    elif augmentation == 'trivial':\n",
    "        augmentation_transform = transforms.TrivialAugmentWide()\n",
    "    elif augmentation == 'mix':\n",
    "        augmentation_transform = transforms.AugMix()\n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Lambda(squareImage), # custom transform to square image before resizing to keep scale\n",
    "            transforms.Resize(img_size),\n",
    "            augmentation_transform,\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Lambda(squareImage),\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Lambda(squareImage),\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    batch_sizes = batch_size\n",
    "    #batchsize and numworkers can be changed to predefined variable for easier logging\n",
    "    data_dir =  '.\\datasets\\degree_256_hands\\dataset_v3'\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                            data_transforms[x])\n",
    "                    for x in ['train', 'val', 'test']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_sizes,\n",
    "                                                shuffle=True, num_workers=4)\n",
    "                for x in ['train', 'val', 'test']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    return (dataloaders, dataset_sizes)\n",
    "\n",
    "\n",
    "def build_network(model_arch = 'resnet18', pretrained = True, dropout = 0.5, batch_norm = True):\n",
    "    if model_arch.startswith('resnet'):\n",
    "        if model_arch == 'resnet18':\n",
    "            if pretrained == True:\n",
    "                model_ft = models.resnet18(weights = models.ResNet18_Weights.DEFAULT)\n",
    "            else:\n",
    "                model_ft = models.resnet18(weights = None)\n",
    "        \n",
    "        elif model_arch == 'resnet34':\n",
    "            if pretrained == True:\n",
    "                model_ft = models.resnet34(weights = models.ResNet34_Weights.DEFAULT)\n",
    "            else:  \n",
    "                model_ft = models.resnet34(weights = None)\n",
    "                \n",
    "        elif model_arch == 'resnet50':\n",
    "            if pretrained == True:\n",
    "                model_ft = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n",
    "            else:  \n",
    "                model_ft = models.resnet50(weights = None)\n",
    "        \n",
    "        elif model_arch == 'resnet101':\n",
    "            if pretrained == True:\n",
    "                model_ft = models.resnet101(weights = models.ResNet101_Weights.DEFAULT)\n",
    "            else:  \n",
    "                model_ft = models.resnet101(weights = None)        \n",
    "        num_classes = 24 \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    elif model_arch.startswith('vgg'):\n",
    "        if model_arch == 'vgg11':\n",
    "            if batch_norm == True:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg11_bn(weights = models.VGG11_BN_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg11_bn(weights = None)\n",
    "                    \n",
    "            elif batch_norm == False:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg11(weights = models.VGG11_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg11(weights = None)               \n",
    "        \n",
    "        elif model_arch == 'vgg13':\n",
    "            if batch_norm == True:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg13_bn(weights = models.VGG13_BN_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg13_bn(weights = None)\n",
    "                    \n",
    "            elif batch_norm == False:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg13(weights = models.VGG13_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg13(weights = None)   \n",
    "                \n",
    "        elif model_arch == 'vgg16':\n",
    "            if batch_norm == True:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg16_bn(weights = models.VGG16_BN_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg16_bn(weights = None)\n",
    "                    \n",
    "            elif batch_norm == False:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg16(weights = models.VGG16_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg16(weights = None)   \n",
    "        \n",
    "        elif model_arch == 'vgg19':\n",
    "            if batch_norm == True:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg19_bn(weights = models.VGG19_BN_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg19_bn(weights = None)\n",
    "                    \n",
    "            elif batch_norm == False:\n",
    "                if pretrained == True:\n",
    "                    model_ft = models.vgg19(weights = models.VGG19_Weights.DEFAULT)\n",
    "                else:\n",
    "                    model_ft = models.vgg19(weights = None)   \n",
    "                      \n",
    "        num_classes = 24 \n",
    "        model_ft.classifier[6] = nn.Linear(4096,num_classes)\n",
    "        \n",
    "    elif model_arch.startswith('alex'):\n",
    "        if pretrained == True:\n",
    "            model_ft = models.alexnet(weights = models.AlexNet_Weights.DEFAULT)\n",
    "        else:\n",
    "            model_ft = models.alexnet(weights = None)\n",
    "            \n",
    "        num_classes = 24 \n",
    "        model_ft.classifier[6] = nn.Linear(4096,num_classes)\n",
    "    \n",
    "    elif model_arch.startswith('squ'):\n",
    "        if pretrained == True:\n",
    "            model_ft = models.squeezenet1_1(weights = models.SqueezeNet1_1_Weights.DEFAULT)\n",
    "        else:\n",
    "            model_ft = models.squeezenet1_1(weights = None)\n",
    "            \n",
    "        num_classes = 24 \n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    \n",
    "    elif model_arch.startswith('ince'):\n",
    "        if pretrained == True:\n",
    "            model_ft = models.inception_v3(weights = models.Inception_V3_Weights.DEFAULT)\n",
    "        else:\n",
    "            model_ft = models.inception_v3(weights = None)\n",
    "            \n",
    "        model_ft.aux_logits=False\n",
    "        num_classes = 24 \n",
    "        model_ft.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "        model_ft.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    elif model_arch.startswith('goo'):\n",
    "        if pretrained == True:\n",
    "            model_ft = models.googlenet(weights = models.GoogLeNet_Weights.DEFAULT)\n",
    "        else:\n",
    "            model_ft = models.googlenet(weights = None)\n",
    "            \n",
    "        model_ft.aux_logits=False\n",
    "        num_classes = 24 \n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    return model_ft.to(device)\n",
    "        \n",
    "\n",
    "def build_optimizer(network, optimizer_type, learning_rate, momentum = 0.9, weight_decay=1e-4):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_type == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_type == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(network.parameters(),\n",
    "                               lr=learning_rate, weight_decay=weight_decay)        \n",
    "    return optimizer\n",
    "\n",
    "def build_scheduler(scheduler_type, optimizer_ft, step_sched = 3, gamma_sched = 0.1):\n",
    "    if scheduler_type == \"ReduceLROnPlateau\":\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', factor=0.1, threshold_mode='rel', min_lr=0, eps=1e-08, verbose=False)\n",
    "\n",
    "    elif scheduler_type == \"ConstantLR\":\n",
    "        scheduler = lr_scheduler.ConstantLR(optimizer_ft, factor=1, last_epoch=-1, verbose=False)\n",
    "        \n",
    "    elif scheduler_type == \"StepLR\":\n",
    "        scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=step_sched, gamma=gamma_sched)\n",
    "\n",
    "    return scheduler    \n",
    "\n",
    "\n",
    "def train_epoch(model, dataloaders, optimizer, scheduler, dataset_sizes, scheduler_type, num_epochs, criterion = nn.CrossEntropyLoss(), ):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                # epoch_lr = scheduler.get_last_lr()\n",
    "                if scheduler_type == \"ReduceLROnPlateau\":\n",
    "                    scheduler.step(running_loss)\n",
    "                else:\n",
    "                    scheduler.step() # running_loss\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                wandb.log({\"Train loss\": epoch_loss}, epoch) # epoch_loss\n",
    "                wandb.log({\"Train accuracy\": epoch_acc}, epoch) #epoch_acc\n",
    "                \n",
    "            if phase == 'val':\n",
    "                wandb.log({\"Validation loss\": epoch_loss}, epoch) # epoch_loss\n",
    "                wandb.log({\"Validation accuracy\": epoch_acc}, epoch) #epoch_acc\n",
    "                \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def test_model(network, loader, dataset_sizes, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    model = network\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in loader['test']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data) \n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes['test']\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes['test']\n",
    "    \n",
    "    return (epoch_loss, epoch_acc)\n",
    "    \n",
    "def confusion_matrix_pass(model, loader):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(loader['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for j in range(inputs.size()[0]):\n",
    "                prediction = int(preds[j])\n",
    "                truth = int(labels.cpu().data[j].int())\n",
    "                predictions.append(prediction)\n",
    "                truths.append(truth)\n",
    "        model.train(mode=was_training)\n",
    "        return (predictions, truths)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Name of sweep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243c3fb731fbda022e9f69d64a196caf4623b12e465908893afa0500898af843"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
